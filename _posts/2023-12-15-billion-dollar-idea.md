---
layout: page
title: About
---

сегодня я проснулся с мыслью, что чат жпт знает все обо всем. не удивительно, что его размер получился на сотни гигабайт.
и следом мне в голову пришла простая и очевидная идея, но почему-то я ее ни от кого не слышал.
суть идеи в том, что мне ведь нафиг не нужно все то, что знает чат жпт. например, его можно попросить называть сотню
древнегреческих мыслителей и он сделает. потом попросить сделать то же самое на японском. и т.д.

мне ведь это все не надо. точнее... можно было бы разделить это. сделать отдельную версию жпт, которая специализируется 
на древней греции, отдельную версию, которая знает японский и т.д.
кстати... кажется это упоминалось в анонсе open-ai, что они хотят делать узкоспециализированных чатботов.
мде. наверное как всегда идея не нова.
но ведь это бомба в том плане, что если повыбрасывать ненужное, то наверняка размер нейронки можно было бы сжать сто раз.
а если бы она весила гигабайт, а не сто, то стала бы доступна большому количеству людей на текущем железе.

вопрос - как выкинуть то, что не нужно. это очень даже интересный вопрос, который было бы здорово обсудить с Максом. жаль,
что его нет теперь рядом. он может быть уже знает какие-то подходящие методы дистиляции.

у меня же есть только очевидные идеи. например, внести шум в какую-то часть или занулить какую-то часть весов и попросить
что-то про японский или греческий. еще идея - добавить шума немного и попробовать дообучать греческому и японскому и посмотреть
в каких частях наибольший градиент, чтобы опять же как-то локализовать области, которые не нужны. еще идея - добавить распад
для весов и продолжить обучать на урезанном объеме данных, т.е. сделать так, чтобы сетка начала забывать какие-то вещи, а потом
выбросить занулившиеся нейроны.
